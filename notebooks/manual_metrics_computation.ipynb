{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Metrics Computation Tutorial\n",
    "\n",
    "This notebook teaches you how to manually compute object detection metrics:\n",
    "- **Precision & Recall**\n",
    "- **IoU (Intersection over Union)**\n",
    "- **AP (Average Precision)**\n",
    "- **mAP (mean Average Precision)**\n",
    "\n",
    "We'll use the validation results from your trained models and compute metrics **with and without background class** to understand the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Validation Results\n",
    "\n",
    "First, let's load the validation metrics that were saved during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the validation metrics from a trained model\n",
    "model_dir = Path('../experiments/model_comparison/retinanet_resnet50')\n",
    "metrics_file = model_dir / 'validation' / 'validation_metrics.json'\n",
    "\n",
    "with open(metrics_file, 'r') as f:\n",
    "    metrics = json.load(f)\n",
    "\n",
    "print(\"Loaded metrics from:\", metrics_file)\n",
    "print(\"\\nAvailable keys:\", list(metrics.keys()))\n",
    "print(\"\\nTop-level metrics:\")\n",
    "for key in ['mAP@0.5', 'mAP@0.5:0.95', 'precision', 'recall', 'f1_score']:\n",
    "    print(f\"  {key}: {metrics[key]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Per-Class Metrics\n",
    "\n",
    "Let's look at the per-class Average Precision (AP) values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPer-class metrics:\")\n",
    "print(f\"{'Class':<10} {'AP@0.5':<12} {'Precision':<12} {'Recall':<12}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "class_aps = []\n",
    "class_names_mapping = {\n",
    "    0: \"Background\",\n",
    "    1: \"bien_bao_hieu_lenh\",\n",
    "    2: \"bien_bao_nguy_hiem\",\n",
    "    3: \"bien_chi_dan\",\n",
    "    4: \"bien_phu\",\n",
    "    5: \"bien_bao_cam\"\n",
    "}\n",
    "\n",
    "for class_key, class_data in metrics['per_class'].items():\n",
    "    class_id = int(class_key.split('_')[1])\n",
    "    ap = class_data['ap']\n",
    "    precision = class_data['precision']\n",
    "    recall = class_data['recall']\n",
    "    \n",
    "    class_name = class_names_mapping.get(class_id, f\"Class {class_id}\")\n",
    "    print(f\"{class_name:<10} {ap:<12.4f} {precision:<12.4f} {recall:<12.4f}\")\n",
    "    class_aps.append((class_id, ap))\n",
    "\n",
    "print(\"\\nâš ï¸ Notice: Class 0 (Background) has AP = 0.0000\")\n",
    "print(\"This is WRONG because background shouldn't be included in evaluation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Manual mAP Computation - INCORRECT (With Background)\n",
    "\n",
    "Let's manually compute mAP the **wrong way** (including background):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INCORRECT mAP Computation (Including Background)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Extract all AP values (including class 0)\n",
    "all_aps = [class_data['ap'] for class_data in metrics['per_class'].values()]\n",
    "\n",
    "print(\"\\nAP values for each class:\")\n",
    "for i, ap in enumerate(all_aps):\n",
    "    print(f\"  Class {i}: {ap:.4f}\")\n",
    "\n",
    "# Compute mean\n",
    "incorrect_mAP = np.mean(all_aps)\n",
    "\n",
    "print(f\"\\nFormula: mAP = (AP_0 + AP_1 + AP_2 + AP_3 + AP_4 + AP_5) / 6\")\n",
    "print(f\"mAP = ({' + '.join([f'{ap:.4f}' for ap in all_aps])}) / {len(all_aps)}\")\n",
    "print(f\"mAP = {sum(all_aps):.4f} / {len(all_aps)}\")\n",
    "print(f\"\\nâŒ Incorrect mAP@0.5 = {incorrect_mAP:.4f}\")\n",
    "print(f\"   (This matches the reported value: {metrics['mAP@0.5']:.4f})\")\n",
    "\n",
    "print(\"\\nâš ï¸ Problem: Class 0 (background) with AP=0.0 artificially lowers the mAP!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Manual mAP Computation - CORRECT (Excluding Background)\n",
    "\n",
    "Now let's compute mAP the **correct way** (excluding background class):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CORRECT mAP Computation (Excluding Background)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Extract AP values for classes 1-5 only (excluding class 0)\n",
    "correct_aps = []\n",
    "for class_key, class_data in metrics['per_class'].items():\n",
    "    class_id = int(class_key.split('_')[1])\n",
    "    if class_id > 0:  # Skip background (class 0)\n",
    "        correct_aps.append(class_data['ap'])\n",
    "\n",
    "print(\"\\nAP values for actual classes (excluding background):\")\n",
    "for i, ap in enumerate(correct_aps, start=1):\n",
    "    print(f\"  Class {i}: {ap:.4f}\")\n",
    "\n",
    "# Compute mean\n",
    "correct_mAP = np.mean(correct_aps)\n",
    "\n",
    "print(f\"\\nFormula: mAP = (AP_1 + AP_2 + AP_3 + AP_4 + AP_5) / 5\")\n",
    "print(f\"mAP = ({' + '.join([f'{ap:.4f}' for ap in correct_aps])}) / {len(correct_aps)}\")\n",
    "print(f\"mAP = {sum(correct_aps):.4f} / {len(correct_aps)}\")\n",
    "print(f\"\\nâœ… Correct mAP@0.5 = {correct_mAP:.4f}\")\n",
    "\n",
    "# Show improvement\n",
    "improvement = ((correct_mAP - incorrect_mAP) / incorrect_mAP) * 100\n",
    "print(f\"\\nðŸ“ˆ Improvement: +{improvement:.1f}% ({correct_mAP:.4f} vs {incorrect_mAP:.4f})\")\n",
    "print(f\"   Absolute gain: +{correct_mAP - incorrect_mAP:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Understanding IoU (Intersection over Union)\n",
    "\n",
    "IoU measures the overlap between predicted and ground truth bounding boxes.\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "IoU = Area of Intersection / Area of Union\n",
    "```\n",
    "\n",
    "Let's implement and visualize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Compute IoU between two bounding boxes\n",
    "    \n",
    "    Args:\n",
    "        box1, box2: [x1, y1, x2, y2] format\n",
    "        \n",
    "    Returns:\n",
    "        IoU value (0 to 1)\n",
    "    \"\"\"\n",
    "    # Get coordinates\n",
    "    x1_min, y1_min, x1_max, y1_max = box1\n",
    "    x2_min, y2_min, x2_max, y2_max = box2\n",
    "    \n",
    "    # Compute intersection\n",
    "    inter_x_min = max(x1_min, x2_min)\n",
    "    inter_y_min = max(y1_min, y2_min)\n",
    "    inter_x_max = min(x1_max, x2_max)\n",
    "    inter_y_max = min(y1_max, y2_max)\n",
    "    \n",
    "    # Check if boxes overlap\n",
    "    if inter_x_max < inter_x_min or inter_y_max < inter_y_min:\n",
    "        return 0.0\n",
    "    \n",
    "    # Compute areas\n",
    "    intersection_area = (inter_x_max - inter_x_min) * (inter_y_max - inter_y_min)\n",
    "    box1_area = (x1_max - x1_min) * (y1_max - y1_min)\n",
    "    box2_area = (x2_max - x2_min) * (y2_max - y2_min)\n",
    "    union_area = box1_area + box2_area - intersection_area\n",
    "    \n",
    "    # Compute IoU\n",
    "    iou = intersection_area / union_area\n",
    "    \n",
    "    return iou\n",
    "\n",
    "\n",
    "# Example boxes\n",
    "print(\"\\nIoU Examples:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Example 1: Perfect overlap\n",
    "box_gt = [10, 10, 50, 50]\n",
    "box_pred = [10, 10, 50, 50]\n",
    "iou = compute_iou(box_gt, box_pred)\n",
    "print(f\"\\n1. Perfect overlap:\")\n",
    "print(f\"   Ground Truth: {box_gt}\")\n",
    "print(f\"   Prediction:   {box_pred}\")\n",
    "print(f\"   IoU = {iou:.4f} (perfect match!)\")\n",
    "\n",
    "# Example 2: Partial overlap\n",
    "box_pred = [20, 20, 60, 60]\n",
    "iou = compute_iou(box_gt, box_pred)\n",
    "print(f\"\\n2. Partial overlap:\")\n",
    "print(f\"   Ground Truth: {box_gt}\")\n",
    "print(f\"   Prediction:   {box_pred}\")\n",
    "print(f\"   IoU = {iou:.4f}\")\n",
    "\n",
    "# Example 3: Small overlap\n",
    "box_pred = [40, 40, 80, 80]\n",
    "iou = compute_iou(box_gt, box_pred)\n",
    "print(f\"\\n3. Small overlap:\")\n",
    "print(f\"   Ground Truth: {box_gt}\")\n",
    "print(f\"   Prediction:   {box_pred}\")\n",
    "print(f\"   IoU = {iou:.4f}\")\n",
    "\n",
    "# Example 4: No overlap\n",
    "box_pred = [100, 100, 140, 140]\n",
    "iou = compute_iou(box_gt, box_pred)\n",
    "print(f\"\\n4. No overlap:\")\n",
    "print(f\"   Ground Truth: {box_gt}\")\n",
    "print(f\"   Prediction:   {box_pred}\")\n",
    "print(f\"   IoU = {iou:.4f}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Key: IoU > 0.5 is typically considered a 'correct' detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Computing Precision and Recall\n",
    "\n",
    "For object detection:\n",
    "\n",
    "**Precision** = TP / (TP + FP)  \n",
    "**Recall** = TP / (TP + FN)\n",
    "\n",
    "Where:\n",
    "- **TP (True Positive)**: Correct detection (IoU â‰¥ threshold)\n",
    "- **FP (False Positive)**: Incorrect detection (IoU < threshold or duplicate)\n",
    "- **FN (False Negative)**: Missed ground truth\n",
    "\n",
    "Let's compute these manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Manual Precision & Recall Computation\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Example scenario for one class\n",
    "print(\"\\nðŸ“‹ Scenario: Detecting traffic signs in 5 images\")\n",
    "print(\"\\nGround Truth:\")\n",
    "print(\"  Image 1: 2 signs\")\n",
    "print(\"  Image 2: 1 sign\")\n",
    "print(\"  Image 3: 3 signs\")\n",
    "print(\"  Image 4: 0 signs\")\n",
    "print(\"  Image 5: 1 sign\")\n",
    "print(\"  Total GT: 7 signs\")\n",
    "\n",
    "print(\"\\nPredictions (with IoU check):\")\n",
    "print(\"  Image 1: 2 detections (both IoU > 0.5) â†’ 2 TP\")\n",
    "print(\"  Image 2: 1 detection (IoU > 0.5) â†’ 1 TP\")\n",
    "print(\"  Image 3: 4 detections (2 with IoU > 0.5, 1 with IoU < 0.5, 1 duplicate) â†’ 2 TP, 2 FP\")\n",
    "print(\"  Image 4: 1 detection (no GT, so IoU = 0) â†’ 1 FP\")\n",
    "print(\"  Image 5: 0 detections â†’ 0 TP (1 FN)\")\n",
    "\n",
    "# Count detections\n",
    "TP = 2 + 1 + 2 + 0 + 0  # True Positives\n",
    "FP = 0 + 0 + 2 + 1 + 0  # False Positives\n",
    "FN = 0 + 0 + 1 + 0 + 1  # False Negatives (missed GT)\n",
    "total_GT = 7\n",
    "\n",
    "print(f\"\\n\\nðŸ“Š Counts:\")\n",
    "print(f\"  True Positives (TP):  {TP}\")\n",
    "print(f\"  False Positives (FP): {FP}\")\n",
    "print(f\"  False Negatives (FN): {FN}\")\n",
    "print(f\"  Total Ground Truth:   {total_GT}\")\n",
    "\n",
    "# Compute metrics\n",
    "precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(f\"\\n\\nðŸ“ˆ Metrics:\")\n",
    "print(f\"  Precision = TP / (TP + FP) = {TP} / ({TP} + {FP}) = {TP} / {TP + FP} = {precision:.4f}\")\n",
    "print(f\"  Recall    = TP / (TP + FN) = {TP} / ({TP} + {FN}) = {TP} / {TP + FN} = {recall:.4f}\")\n",
    "print(f\"  F1 Score  = 2 * (P * R) / (P + R) = 2 * ({precision:.4f} * {recall:.4f}) / ({precision:.4f} + {recall:.4f}) = {f1:.4f}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Interpretation:\")\n",
    "print(f\"  - {precision*100:.1f}% of detections are correct\")\n",
    "print(f\"  - We found {recall*100:.1f}% of all ground truth signs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Computing Average Precision (AP)\n",
    "\n",
    "AP summarizes the precision-recall curve. Here's how to compute it:\n",
    "\n",
    "1. Sort predictions by confidence score (highest first)\n",
    "2. For each prediction, compute precision and recall\n",
    "3. Plot precision-recall curve\n",
    "4. Compute area under curve (AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ap(precisions, recalls):\n",
    "    \"\"\"\n",
    "    Compute Average Precision using 11-point interpolation\n",
    "    \n",
    "    Args:\n",
    "        precisions: Array of precision values\n",
    "        recalls: Array of recall values\n",
    "        \n",
    "    Returns:\n",
    "        Average Precision\n",
    "    \"\"\"\n",
    "    # 11-point interpolation\n",
    "    ap = 0.0\n",
    "    for threshold in np.linspace(0, 1, 11):\n",
    "        # Get precision at recall >= threshold\n",
    "        precisions_at_recall = precisions[recalls >= threshold]\n",
    "        if len(precisions_at_recall) > 0:\n",
    "            ap += np.max(precisions_at_recall)\n",
    "    \n",
    "    return ap / 11.0\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Computing Average Precision (AP)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Example: Predictions for one class sorted by confidence\n",
    "predictions = [\n",
    "    {'confidence': 0.95, 'iou': 0.8, 'correct': True},   # TP\n",
    "    {'confidence': 0.90, 'iou': 0.7, 'correct': True},   # TP\n",
    "    {'confidence': 0.85, 'iou': 0.3, 'correct': False},  # FP\n",
    "    {'confidence': 0.80, 'iou': 0.6, 'correct': True},   # TP\n",
    "    {'confidence': 0.75, 'iou': 0.2, 'correct': False},  # FP\n",
    "    {'confidence': 0.70, 'iou': 0.5, 'correct': True},   # TP\n",
    "]\n",
    "total_gt = 6  # Total ground truth objects\n",
    "\n",
    "print(\"\\nPredictions (sorted by confidence):\")\n",
    "print(f\"{'#':<4} {'Conf':<8} {'IoU':<8} {'Type':<8} {'TP':<6} {'FP':<6} {'Precision':<12} {'Recall':<12}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "tp_cumsum = 0\n",
    "fp_cumsum = 0\n",
    "precisions_list = []\n",
    "recalls_list = []\n",
    "\n",
    "for i, pred in enumerate(predictions, 1):\n",
    "    if pred['correct']:\n",
    "        tp_cumsum += 1\n",
    "        pred_type = 'TP'\n",
    "    else:\n",
    "        fp_cumsum += 1\n",
    "        pred_type = 'FP'\n",
    "    \n",
    "    precision = tp_cumsum / (tp_cumsum + fp_cumsum)\n",
    "    recall = tp_cumsum / total_gt\n",
    "    \n",
    "    precisions_list.append(precision)\n",
    "    recalls_list.append(recall)\n",
    "    \n",
    "    print(f\"{i:<4} {pred['confidence']:<8.2f} {pred['iou']:<8.2f} {pred_type:<8} \"\n",
    "          f\"{tp_cumsum:<6} {fp_cumsum:<6} {precision:<12.4f} {recall:<12.4f}\")\n",
    "\n",
    "# Compute AP\n",
    "precisions_array = np.array(precisions_list)\n",
    "recalls_array = np.array(recalls_list)\n",
    "ap = compute_ap(precisions_array, recalls_array)\n",
    "\n",
    "print(f\"\\nðŸ“Š Average Precision (AP) = {ap:.4f}\")\n",
    "\n",
    "# Plot precision-recall curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(recalls_list, precisions_list, 'b-o', linewidth=2, markersize=8, label='Precision-Recall curve')\n",
    "plt.xlabel('Recall', fontsize=12)\n",
    "plt.ylabel('Precision', fontsize=12)\n",
    "plt.title(f'Precision-Recall Curve (AP = {ap:.4f})', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1.05])\n",
    "plt.legend(fontsize=10)\n",
    "\n",
    "# Shade area under curve\n",
    "plt.fill_between(recalls_list, precisions_list, alpha=0.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ The area under this curve is the Average Precision (AP)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize mAP Comparison\n",
    "\n",
    "Let's visualize the difference between correct and incorrect mAP computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract class APs\n",
    "class_names = ['Background', 'bien_bao_hieu_lenh', 'bien_bao_nguy_hiem', 'bien_chi_dan', 'bien_phu', 'bien_bao_cam']\n",
    "class_aps_with_bg = [metrics['per_class'][f'class_{i}']['ap'] for i in range(6)]\n",
    "class_aps_without_bg = class_aps_with_bg[1:]  # Exclude class 0\n",
    "\n",
    "# Create comparison plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Left: With background\n",
    "x1 = np.arange(len(class_aps_with_bg))\n",
    "colors1 = ['red'] + ['green'] * 5\n",
    "bars1 = ax1.bar(x1, class_aps_with_bg, color=colors1, alpha=0.7, edgecolor='black')\n",
    "ax1.set_xlabel('Class ID', fontsize=12)\n",
    "ax1.set_ylabel('Average Precision (AP)', fontsize=12)\n",
    "ax1.set_title(f'âŒ INCORRECT: mAP with Background\\nmAP@0.5 = {incorrect_mAP:.4f}', fontsize=14)\n",
    "ax1.set_xticks(x1)\n",
    "ax1.set_xticklabels([f'{i}' for i in range(6)])\n",
    "ax1.axhline(y=incorrect_mAP, color='blue', linestyle='--', linewidth=2, label=f'mAP = {incorrect_mAP:.4f}')\n",
    "ax1.set_ylim([0, 1.05])\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "ax1.legend(fontsize=10)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars1, class_aps_with_bg):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "             f'{val:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Annotate background class\n",
    "ax1.text(0, -0.15, 'â† Background\\n(should not\\nbe included!)', ha='center', fontsize=10, color='red', weight='bold')\n",
    "\n",
    "# Right: Without background\n",
    "x2 = np.arange(len(class_aps_without_bg))\n",
    "bars2 = ax2.bar(x2, class_aps_without_bg, color='green', alpha=0.7, edgecolor='black')\n",
    "ax2.set_xlabel('Class ID (remapped)', fontsize=12)\n",
    "ax2.set_ylabel('Average Precision (AP)', fontsize=12)\n",
    "ax2.set_title(f'âœ… CORRECT: mAP without Background\\nmAP@0.5 = {correct_mAP:.4f}', fontsize=14)\n",
    "ax2.set_xticks(x2)\n",
    "ax2.set_xticklabels([f'{i}' for i in range(5)])\n",
    "ax2.axhline(y=correct_mAP, color='blue', linestyle='--', linewidth=2, label=f'mAP = {correct_mAP:.4f}')\n",
    "ax2.set_ylim([0, 1.05])\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "ax2.legend(fontsize=10)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars2, class_aps_without_bg):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "             f'{val:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../experiments/map_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Improvement by excluding background: +{improvement:.1f}%\")\n",
    "print(f\"   From {incorrect_mAP:.4f} to {correct_mAP:.4f}\")\n",
    "print(f\"\\nðŸ’¾ Plot saved to: experiments/map_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Takeaways\n",
    "\n",
    "### âœ… Correct Metrics Computation:\n",
    "\n",
    "1. **Filter background class**: Remove predictions with label = 0\n",
    "2. **Remap labels**: Convert 1â†’0, 2â†’1, 3â†’2, etc.\n",
    "3. **Compute metrics over N-1 classes**: If model has N classes (including background), compute mAP over N-1 classes\n",
    "\n",
    "### ðŸ“Š Formulas Summary:\n",
    "\n",
    "```python\n",
    "# IoU\n",
    "IoU = Intersection_Area / Union_Area\n",
    "\n",
    "# Precision & Recall\n",
    "Precision = TP / (TP + FP)\n",
    "Recall = TP / (TP + FN)\n",
    "F1 = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "# Average Precision (AP)\n",
    "AP = Area_Under_Precision_Recall_Curve\n",
    "\n",
    "# mean Average Precision (mAP)\n",
    "mAP = (AP_class1 + AP_class2 + ... + AP_classN) / N\n",
    "# Note: N should NOT include background class!\n",
    "```\n",
    "\n",
    "### ðŸŽ¯ Your Models' True Performance:\n",
    "\n",
    "Run the `recompute_metrics.py` script to get corrected metrics for all your trained models!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
